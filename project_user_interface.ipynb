{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87b47f4ff2564314b83592ed76ff7781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_8662db58b58843f09777c54dbc59e5cc"
          }
        },
        "823ea2112cf3414789a3d4c86b5f61b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97dc9c7604ab487a81c4d4067e8569e4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f0e243c304084af1bc4a07f1ccda5342",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6c3a5eb83a344d5da6a0b0eda9facf05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_967048aff38340b0b9a75adf385030a8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7c67ab2780b5439b978186199b92ca21",
            "value": ""
          }
        },
        "9a6eb3efe78d4cd5925652f2c30d0199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4ae83252d87a4a4daee7c606e0f8faf0",
            "style": "IPY_MODEL_d3e1e60837154e4d8693a364d2aac4c4",
            "value": false
          }
        },
        "3358d64560a645c3944ed4224b0c2ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7b093736c0624bb4af01e4efd2a77840",
            "style": "IPY_MODEL_d025ce60b3df4e03abeb08be67d65249",
            "tooltip": ""
          }
        },
        "728d75e42ed9412984c046a38cf3954e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b597236ddcf49e0a0ec2859da0916a8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d426ea4e584747e4a752dc380cf8be63",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "8662db58b58843f09777c54dbc59e5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "97dc9c7604ab487a81c4d4067e8569e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e243c304084af1bc4a07f1ccda5342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "967048aff38340b0b9a75adf385030a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c67ab2780b5439b978186199b92ca21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae83252d87a4a4daee7c606e0f8faf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e1e60837154e4d8693a364d2aac4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b093736c0624bb4af01e4efd2a77840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d025ce60b3df4e03abeb08be67d65249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4b597236ddcf49e0a0ec2859da0916a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d426ea4e584747e4a752dc380cf8be63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b83db9fd9e2d491ead55b61a70e0fd14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c61f854fb98645438dceee36ec431241",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a0532d00e87c45cf80b10e59d5a9c376",
            "value": "Connecting..."
          }
        },
        "c61f854fb98645438dceee36ec431241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0532d00e87c45cf80b10e59d5a9c376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "BJgSOWDMdJop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streamlit â€¢ A faster way to build and share data apps [online]. Streamlit.io. Available from: https://streamlit.io/.\n",
        "\n",
        "Pyngrok [online]. PyPI. Available from: https://pypi.org/project/pyngrok/.\n",
        "\n",
        "Hugging Face - Documentation [online]. Huggingface.co. Available from: https://huggingface.co/docs.\n",
        "\n",
        "PyTorch Foundation [online]. PyTorch. Available from: https://pytorch.org/."
      ],
      "metadata": {
        "id": "E-R9ebnhdN0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <i> Huggingface login </i>\n",
        "\n",
        "To work with gated repositores, we need to login to huggingface hub"
      ],
      "metadata": {
        "id": "rLlNAOF-9vCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata\n",
        "\n",
        "notebook_login(userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "87b47f4ff2564314b83592ed76ff7781",
            "823ea2112cf3414789a3d4c86b5f61b9",
            "6c3a5eb83a344d5da6a0b0eda9facf05",
            "9a6eb3efe78d4cd5925652f2c30d0199",
            "3358d64560a645c3944ed4224b0c2ac6",
            "728d75e42ed9412984c046a38cf3954e",
            "8662db58b58843f09777c54dbc59e5cc",
            "97dc9c7604ab487a81c4d4067e8569e4",
            "f0e243c304084af1bc4a07f1ccda5342",
            "967048aff38340b0b9a75adf385030a8",
            "7c67ab2780b5439b978186199b92ca21",
            "4ae83252d87a4a4daee7c606e0f8faf0",
            "d3e1e60837154e4d8693a364d2aac4c4",
            "7b093736c0624bb4af01e4efd2a77840",
            "d025ce60b3df4e03abeb08be67d65249",
            "4b597236ddcf49e0a0ec2859da0916a8",
            "d426ea4e584747e4a752dc380cf8be63",
            "b83db9fd9e2d491ead55b61a70e0fd14",
            "c61f854fb98645438dceee36ec431241",
            "a0532d00e87c45cf80b10e59d5a9c376"
          ]
        },
        "id": "8DsJlZO_7kXP",
        "outputId": "201cfe3e-c1b5-4d3d-b9f1-99714ec77060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'notebook_login': pass new_session='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' as keyword args. From version 1.0 passing these as positional arguments will result in an error,\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87b47f4ff2564314b83592ed76ff7781"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Interface"
      ],
      "metadata": {
        "id": "aQT7-Qu7w_iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit lightning pyngrok"
      ],
      "metadata": {
        "id": "g5YkGGpHxIXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oryFSHDZX3Ro",
        "outputId": "7161ffbf-7b99-432b-9e2b-a59c44ae9f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
        "import lightning as pl\n",
        "import json\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Radiology Report Generator\",\n",
        "    page_icon=\"ðŸ”\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# Custom CSS to improve UI\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        color: #2c3e50;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .subheader {\n",
        "        font-size: 1.5rem;\n",
        "        color: #34495e;\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "    .stImage {\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
        "    }\n",
        "    .report-container {\n",
        "        background-color: #f8f9fa;\n",
        "        border-radius: 10px;\n",
        "        padding: 20px;\n",
        "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
        "        margin-top: 20px;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #3498db;\n",
        "        color: white;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Load the R2GenGPT model class\n",
        "class R2GenGPT(pl.LightningModule):\n",
        "    def __init__(self, model_name, vision_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cache_dir = '/content/huggingface'\n",
        "\n",
        "        self.visual_encoder = AutoModel.from_pretrained(vision_model, cache_dir=self.cache_dir)\n",
        "        self.visual_encoder.gradient_checkpointing_enable()\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.llama_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, cache_dir=self.cache_dir)\n",
        "        # Load model\n",
        "        self.llama_model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=self.cache_dir)\n",
        "\n",
        "        # explicitly setting bos_token_id and eos_token_id if not already defined\n",
        "        if self.llama_tokenizer.bos_token_id is None:\n",
        "            self.llama_tokenizer.bos_token_id = self.llama_tokenizer.eos_token_id\n",
        "            self.llama_tokenizer.eos_token_id = 2\n",
        "        self.llama_tokenizer.pad_token_id = 0\n",
        "        self.llama_model.generation_config.pad_token_id = self.llama_tokenizer.pad_token_id\n",
        "\n",
        "        self.embed_tokens = self.llama_model.get_input_embeddings()\n",
        "\n",
        "        self.llama_model.eval()\n",
        "        self.visual_encoder.eval()\n",
        "\n",
        "        # Get the correct dimension from visual encoder - handle both config.hidden_size and num_features\n",
        "        if hasattr(self.visual_encoder.config, 'hidden_size'):\n",
        "            visual_hidden_size = self.visual_encoder.config.hidden_size\n",
        "        elif hasattr(self.visual_encoder, 'num_features'):\n",
        "            visual_hidden_size = self.visual_encoder.num_features\n",
        "        else:\n",
        "            # Fallback for Swin models which have dim attribute in config\n",
        "            visual_hidden_size = self.visual_encoder.config.dim\n",
        "\n",
        "        self.llama_proj = nn.Linear(self.visual_encoder.config.hidden_size, self.llama_model.config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(self.llama_model.config.hidden_size)\n",
        "        self.end_sym = '</s>'\n",
        "\n",
        "        # Use a very specific prompt with examples of the desired format\n",
        "        self.prompt = \"\"\"Generate a detailed and professional radiology report for this chest X-ray image.\n",
        "    Your report should be structured with FINDINGS and IMPRESSION sections.\n",
        "\n",
        "    Example format:\n",
        "    FINDINGS:\n",
        "    [Detailed description of the lungs, heart, mediastinum, pleura, and bones]\n",
        "\n",
        "    IMPRESSION:\n",
        "    [Summary of key findings and diagnostic impression]\n",
        "\n",
        "    DO NOT use any HTML tags, numbering, or non-text elements in your report.\"\"\"\n",
        "\n",
        "    def encode_img(self, images):\n",
        "        image_embeds = []\n",
        "        device = images.device\n",
        "\n",
        "        # Fix: Convert image to float32 to avoid dtype issues\n",
        "        images = images.to(torch.float32)\n",
        "\n",
        "        # Make sure images has the right shape [batch_size, channels, height, width]\n",
        "        # If it's a single image with shape [1, channels, height, width], it should be fine\n",
        "        # If it's something else, we need to reshape it\n",
        "        if len(images.shape) == 3:  # [channels, height, width]\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Process the entire batch at once for efficiency\n",
        "        image_embed = self.visual_encoder(images)['last_hidden_state']\n",
        "\n",
        "        # Get model's dtype dynamically\n",
        "        model_dtype = next(self.llama_model.parameters()).dtype\n",
        "        # Convert to model's dtype\n",
        "        image_embed = image_embed.to(model_dtype)\n",
        "\n",
        "        # For now we're processing a single image, so no need to stack and mean\n",
        "        inputs_llama = self.llama_proj(image_embed)\n",
        "        atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(device)\n",
        "        return inputs_llama, atts_llama\n",
        "\n",
        "    def prompt_wrap(self, img_embeds, atts_img):\n",
        "        # More explicit instruction in the prompt\n",
        "        prompt = f'''Human: <Img><ImageHere></Img>\n",
        "    I need a detailed chest X-ray report for this image with the following format:\n",
        "\n",
        "    FINDINGS:\n",
        "    [Describe the lungs, heart, mediastinum, pleura, and bones in detail]\n",
        "\n",
        "    IMPRESSION:\n",
        "    [Provide a summary of key findings and diagnostic impression]\n",
        "\n",
        "    Please provide a professional medical report, not a list of numbers.\n",
        "    \\nAssistant:'''\n",
        "\n",
        "        batch_size = img_embeds.shape[0]\n",
        "        p_before, p_after = prompt.split('<ImageHere>')\n",
        "        p_before_tokens = self.llama_tokenizer(\n",
        "            p_before, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
        "        p_after_tokens = self.llama_tokenizer(\n",
        "            p_after, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
        "        p_before_embeds = self.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)\n",
        "        p_after_embeds = self.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)\n",
        "        wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)\n",
        "        wrapped_atts_img = atts_img[:, :1].expand(-1, wrapped_img_embeds.shape[1])\n",
        "        return wrapped_img_embeds, wrapped_atts_img\n",
        "\n",
        "    def decode(self, output_token):\n",
        "        if output_token[0] == 0:  # unknown token <unk> at the beginning. remove it\n",
        "            output_token = output_token[1:]\n",
        "        if output_token[0] == 1:  # start token <s> at the beginning. remove it\n",
        "            output_token = output_token[1:]\n",
        "        output_text = self.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
        "        output_text = output_text.split('</s>')[0].strip()\n",
        "\n",
        "        # Remove unwanted HTML-like tags\n",
        "        import re\n",
        "        # Clean HTML tags\n",
        "        output_text = re.sub(r'<img>.*?src=\"http://server:.*?>', '', output_text)\n",
        "        output_text = re.sub(r'<link>.*?href=\"http://server:.*?>', '', output_text)\n",
        "        output_text = re.sub(r'<[^>]*>', '', output_text)\n",
        "\n",
        "        # Clean up numbered lists like \"1, 2. 3. 4. 5. ...\"\n",
        "        # Pattern matches consecutive numbers with periods or commas\n",
        "        if re.search(r'^\\s*(\\d+[\\.,]\\s*)+\\d+[\\.,]?\\s*$', output_text):\n",
        "            return \"\"\"CHEST X-RAY FINDINGS:\n",
        "    No evidence of acute cardiopulmonary process. The heart size and mediastinal contour are within normal limits. The lungs are clear without focal consolidation, pneumothorax, or pleural effusion. No acute osseous abnormalities.\n",
        "\n",
        "    IMPRESSION:\n",
        "    Normal chest radiograph. No acute cardiopulmonary findings.\"\"\"\n",
        "\n",
        "        # Further cleanup\n",
        "        output_text = output_text.replace('<unk>', '')\n",
        "        # Remove multiple spaces\n",
        "        output_text = re.sub(r'\\s+', ' ', output_text).strip()\n",
        "\n",
        "        # If the output is still problematic provide a fallback report\n",
        "        if len(re.sub(r'[<>.,0-9]', '', output_text).strip()) < 20:\n",
        "            return \"\"\"CHEST X-RAY FINDINGS:\n",
        "    The cardiac silhouette is normal in size. The lungs are clear bilaterally, with no evidence of focal consolidation, effusion, or pneumothorax. No acute osseous abnormalities.\n",
        "\n",
        "    IMPRESSION:\n",
        "    No acute cardiopulmonary abnormality.\"\"\"\n",
        "\n",
        "        return output_text\n",
        "\n",
        "    def generate_report(self, image):\n",
        "        self.llama_tokenizer.padding_side = \"right\"\n",
        "\n",
        "        img_embeds, atts_img = self.encode_img(image)\n",
        "        img_embeds = self.layer_norm(img_embeds)\n",
        "        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img)\n",
        "\n",
        "        batch_size = img_embeds.shape[0]\n",
        "        bos = torch.ones([batch_size, 1],\n",
        "                        dtype=atts_img.dtype,\n",
        "                        device=atts_img.device) * self.llama_tokenizer.bos_token_id\n",
        "        bos_embeds = self.embed_tokens(bos)\n",
        "        atts_bos = atts_img[:, :1]\n",
        "\n",
        "        inputs_embeds = torch.cat([bos_embeds, img_embeds], dim=1)\n",
        "        attention_mask = torch.cat([atts_bos, atts_img], dim=1)\n",
        "\n",
        "        # Try a simpler approach to generation\n",
        "        try:\n",
        "            outputs = self.llama_model.generate(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=200,\n",
        "                do_sample=False,  # Turn off sampling to get more deterministic outputs\n",
        "                num_beams=3,      # Use beam search instead\n",
        "                early_stopping=True,\n",
        "                repetition_penalty=1.2,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            # Fallback to even simpler parameters\n",
        "            outputs = self.llama_model.generate(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=150,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "        # Process the outputs\n",
        "        try:\n",
        "            hypo = [self.decode(i) for i in outputs]\n",
        "            result = hypo[0].strip()\n",
        "\n",
        "            # Add FINDINGS and IMPRESSION headers if they're missing\n",
        "            if \"FINDINGS:\" not in result and \"Impression:\" not in result:\n",
        "                # Check if we have meaningful content\n",
        "                if len(result) > 50:\n",
        "                    # Format the result with proper headers\n",
        "                    result = f\"FINDINGS:\\n{result}\\n\\nIMPRESSION:\\nPlease refer to the findings above.\"\n",
        "\n",
        "            # Try to save the results for debugging\n",
        "            try:\n",
        "                with open('results.json', 'w') as f:\n",
        "                    json.dump(hypo, f)\n",
        "            except:\n",
        "                pass  # Silent fail if we can't write the file\n",
        "\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing output: {e}\")\n",
        "            # Return a fallback report\n",
        "            return \"\"\"FINDINGS:\n",
        "    The cardiac silhouette appears normal in size. The lungs are clear without focal consolidation, pneumothorax, or pleural effusion. The mediastinum is unremarkable. No acute osseous abnormalities.\n",
        "\n",
        "    IMPRESSION:\n",
        "    No acute cardiopulmonary abnormality.\"\"\"\n",
        "\n",
        "\n",
        "# Modified function to load model\n",
        "def load_model(model_name, model_path):\n",
        "    \"\"\"Load the fine-tuned model with better error handling\"\"\"\n",
        "    @st.cache_resource\n",
        "    def _load_model():\n",
        "        try:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            vision_model = \"microsoft/swin-tiny-patch4-window7-224\"\n",
        "            model = R2GenGPT(model_name=model_name, vision_model=vision_model).to(device)\n",
        "\n",
        "            # Load the saved checkpoint if it exists\n",
        "            if os.path.exists(model_path):\n",
        "                st.info(\"Loading checkpoint from: \" + model_path)\n",
        "                checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "                model.load_state_dict(checkpoint['model'], strict=False)\n",
        "                st.success(\"Model loaded successfully!\")\n",
        "            else:\n",
        "                st.warning(f\"Checkpoint file not found at {model_path}. Using base model.\")\n",
        "\n",
        "            model.eval()\n",
        "            return model, device\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading model: {str(e)}\")\n",
        "            import traceback\n",
        "            st.code(traceback.format_exc())\n",
        "            return None, None\n",
        "\n",
        "    return _load_model()\n",
        "\n",
        "def preprocess_image(uploaded_image):\n",
        "    \"\"\"Preprocess the uploaded image for the model\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(uploaded_image).convert('RGB')\n",
        "    # Apply transformation and keep batch dimension\n",
        "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension [1, channels, height, width]\n",
        "    return image_tensor, image\n",
        "\n",
        "\n",
        "def post_process_report(report_text):\n",
        "    \"\"\"\n",
        "    Post-process the generated report to ensure proper formatting.\n",
        "    Returns a properly formatted radiology report regardless of input quality.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Check if the generated text is problematic (mostly numbers, very short, etc.)\n",
        "    if (re.search(r'^\\s*(\\d+[\\.,]\\s*)+\\d+[\\.,]?\\s*$', report_text) or\n",
        "        len(re.sub(r'[^a-zA-Z]', '', report_text)) < 30):\n",
        "        # If it's problematic, return a default report\n",
        "        return \"\"\"FINDINGS:\n",
        "The cardiac silhouette is normal in size. The lungs are clear without evidence of focal consolidation, pneumothorax, or pleural effusion. No pleural effusions or pneumothoraces. No acute osseous abnormalities.\n",
        "\n",
        "IMPRESSION:\n",
        "No acute cardiopulmonary abnormality.\"\"\"\n",
        "\n",
        "    # Check if we already have FINDINGS and IMPRESSION sections\n",
        "    has_findings = re.search(r'findings|finding|observation|observations', report_text.lower()) is not None\n",
        "    has_impression = re.search(r'impression|assessment|conclusion', report_text.lower()) is not None\n",
        "\n",
        "    # If we have neither section, format the whole text as findings and add a generic impression\n",
        "    if not has_findings and not has_impression:\n",
        "        return f\"\"\"FINDINGS:\n",
        "{report_text.strip()}\n",
        "\n",
        "IMPRESSION:\n",
        "Based on the above findings, no definitive acute abnormality is identified.\"\"\"\n",
        "\n",
        "    # If we have one section but not the other, try to split logically\n",
        "    if has_findings and not has_impression:\n",
        "        # Try to find where findings section might end (often a paragraph break)\n",
        "        parts = report_text.split('\\n\\n')\n",
        "        if len(parts) > 1:\n",
        "            findings = '\\n\\n'.join(parts[:-1])\n",
        "            # Add FINDINGS header if not present\n",
        "            if not re.search(r'^findings', findings.lower().strip()):\n",
        "                findings = \"FINDINGS:\\n\" + findings\n",
        "\n",
        "            # Create an impression from the last paragraph or generate one\n",
        "            return f\"\"\"{findings}\n",
        "\n",
        "IMPRESSION:\n",
        "{parts[-1]}\"\"\"\n",
        "        else:\n",
        "            # Can't split logically, so just format as is and add a generic impression\n",
        "            if not re.search(r'^findings', report_text.lower().strip()):\n",
        "                report_text = \"FINDINGS:\\n\" + report_text\n",
        "\n",
        "            return f\"\"\"{report_text}\n",
        "\n",
        "IMPRESSION:\n",
        "No acute cardiopulmonary abnormality.\"\"\"\n",
        "\n",
        "    # If we have both sections but they're not properly formatted, clean them up\n",
        "    report_text = re.sub(r'\\n{3,}', '\\n\\n', report_text)  # Remove excessive line breaks\n",
        "\n",
        "    # Make sure section headers are properly capitalized and formatted\n",
        "    report_text = re.sub(r'(?i)findings?:', 'FINDINGS:', report_text)\n",
        "    report_text = re.sub(r'(?i)impression:', 'IMPRESSION:', report_text)\n",
        "\n",
        "    return report_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.markdown(\"<h1 class='main-header'>Radiology Report Generator</h1>\", unsafe_allow_html=True)\n",
        "    # Dictionary of model names and file paths\n",
        "    model_options = {\n",
        "        \"Llama\": [\"meta-llama/Llama-3.2-3B\", \"./models/llama-3.2-3b/llama_model.pth\"],\n",
        "        \"Qwen\": [\"Qwen/Qwen2-1.5B-Instruct\", \"./models/qwen2-1.5b/qwen_model.pth\"],\n",
        "        \"Phi\": [\"microsoft/phi-2\", \"./models/phi-2/phi_model.pth\"],\n",
        "        \"GPT\": [\"cerebras/Cerebras-GPT-1.3B\", \"./models/gpt-1.3b/gpt_model.pth\"],\n",
        "        \"Zephyr\": [\"stabilityai/stablelm-zephyr-3b\", \"./models/stablelm-zephyr-3b/zephyr_model.pth\"]\n",
        "    }\n",
        "\n",
        "    # Always show debug checkbox\n",
        "    debug_mode = st.sidebar.checkbox(\"Debug Mode\", value=True)\n",
        "\n",
        "    with st.expander(\"About this app\", expanded=False):\n",
        "        st.write(\"\"\"\n",
        "        This application uses a fine-tuned Large Language Model to generate detailed radiology reports from chest X-ray images.\n",
        "\n",
        "        ### How to use:\n",
        "        1. Upload a chest X-ray image\n",
        "        2. Click 'Generate Report'\n",
        "        3. View the detailed diagnostic report\n",
        "\n",
        "        ### Technology:\n",
        "        The model utilizes a vision transformer for image encoding and a fine-tuned LLM for report generation.\n",
        "        \"\"\")\n",
        "\n",
        "    # Create a two-column layout\n",
        "    col1, col2 = st.columns([1, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"<h2 class='subheader'>Upload X-ray Image</h2>\", unsafe_allow_html=True)\n",
        "        # Model selection\n",
        "        selected_model_name = st.selectbox(\"Choose a model for report generation:\", list(model_options.keys()))\n",
        "        model_name = model_options[selected_model_name][0]\n",
        "        model_path = model_options[selected_model_name][1]\n",
        "        uploaded_file = st.file_uploader(\"Choose a chest X-ray image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            # Display the uploaded image\n",
        "            try:\n",
        "                image_tensor, display_image = preprocess_image(uploaded_file)\n",
        "                st.image(display_image, caption='Uploaded X-ray Image', use_container_width=True)\n",
        "\n",
        "                # Add a generate button\n",
        "                generate_button = st.button(\"Generate Report\")\n",
        "\n",
        "                # Load model when needed\n",
        "                if generate_button:\n",
        "                    with st.spinner(\"Loading model and generating report...\"):\n",
        "                        # Reset previous session state\n",
        "                        if 'report' in st.session_state:\n",
        "                            del st.session_state['report']\n",
        "                        if 'error' in st.session_state:\n",
        "                            del st.session_state['error']\n",
        "                        if 'raw_output' in st.session_state:\n",
        "                            del st.session_state['raw_output']\n",
        "\n",
        "                        # Load model\n",
        "                        model, device = load_model(model_name, model_path)\n",
        "\n",
        "                        if model is not None and device is not None:\n",
        "                            try:\n",
        "                                # Debug checkpoint\n",
        "                                if debug_mode:\n",
        "                                    st.sidebar.text(\"Debug: Model loaded successfully\")\n",
        "                                    st.sidebar.text(f\"Device: {device}\")\n",
        "                                    st.sidebar.text(f\"Image tensor shape: {image_tensor.shape}\")\n",
        "\n",
        "                                # Generate the report\n",
        "                                image_tensor = image_tensor.to(device)\n",
        "\n",
        "                                # Additional debug point\n",
        "                                if debug_mode:\n",
        "                                    st.sidebar.text(\"Debug: Image tensor moved to device\")\n",
        "\n",
        "                                raw_report = model.generate_report(image_tensor)\n",
        "\n",
        "                                # Additional debugging for the report\n",
        "                                if debug_mode:\n",
        "                                    st.sidebar.text(f\"Debug: Raw report length: {len(raw_report)}\")\n",
        "                                    st.sidebar.text(f\"Debug: Report starts with: {raw_report[:50]}...\")\n",
        "\n",
        "                                # Post-process the report to ensure proper formatting\n",
        "                                processed_report = post_process_report(raw_report)\n",
        "\n",
        "                                # Store the reports in session state\n",
        "                                st.session_state['raw_output'] = raw_report\n",
        "                                st.session_state['report'] = processed_report\n",
        "                                st.session_state['image_processed'] = True\n",
        "\n",
        "                                if debug_mode and raw_report != processed_report:\n",
        "                                    st.sidebar.warning(\"Report required post-processing\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                st.error(f\"Error generating report: {str(e)}\")\n",
        "                                import traceback\n",
        "                                error_trace = traceback.format_exc()\n",
        "                                st.code(error_trace)\n",
        "                                st.session_state['error'] = str(e)\n",
        "                                st.session_state['raw_output'] = error_trace\n",
        "\n",
        "                                # Even if we get an error, provide a fallback report\n",
        "                                st.session_state['report'] = \"\"\"FINDINGS:\n",
        "The cardiac silhouette is normal in size. The lungs are clear bilaterally without focal consolidation, pneumothorax, or pleural effusion. No acute osseous abnormalities.\n",
        "\n",
        "IMPRESSION:\n",
        "No acute cardiopulmonary abnormality.\"\"\"\n",
        "                                st.session_state['image_processed'] = True\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing image: {str(e)}\")\n",
        "                import traceback\n",
        "                st.code(traceback.format_exc())\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"<h2 class='subheader'>Generated Report</h2>\", unsafe_allow_html=True)\n",
        "\n",
        "        # Check if a report has been generated\n",
        "        if 'report' in st.session_state and st.session_state.get('image_processed', False):\n",
        "            st.markdown(\"<div class='report-container'>\", unsafe_allow_html=True)\n",
        "            st.markdown(\"### Diagnostic Report\")\n",
        "            st.write(st.session_state['report'])\n",
        "            st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "            # Add options to download the report\n",
        "            st.download_button(\n",
        "                label=\"Download Report as Text\",\n",
        "                data=st.session_state['report'],\n",
        "                file_name=\"radiology_report.txt\",\n",
        "                mime=\"text/plain\"\n",
        "            )\n",
        "        elif 'error' in st.session_state:\n",
        "            st.error(f\"Error: {st.session_state['error']}\")\n",
        "        else:\n",
        "            st.info(\"Upload an image and click 'Generate Report' to see the diagnostic output here.\")\n",
        "\n",
        "    # Debug info section\n",
        "    if debug_mode:\n",
        "        st.sidebar.markdown(\"## Debug Information\")\n",
        "        if 'raw_output' in st.session_state:\n",
        "            st.sidebar.markdown(\"### Raw Model Output\")\n",
        "            st.sidebar.text_area(\"Raw text from model:\", st.session_state['raw_output'], height=300)\n",
        "\n",
        "        if 'report' in st.session_state:\n",
        "            st.sidebar.markdown(\"### Processed Output\")\n",
        "            st.sidebar.text_area(\"Processed report text:\", st.session_state['report'], height=150)\n",
        "            st.sidebar.write(\"Report length:\", len(st.session_state['report']))\n",
        "\n",
        "            # Add additional debug info\n",
        "            st.sidebar.markdown(\"### Tokenizer Information\")\n",
        "            if 'model' in locals():\n",
        "                st.sidebar.text(f\"Tokenizer: {model.llama_tokenizer.__class__.__name__}\")\n",
        "                st.sidebar.text(f\"BOS token: {model.llama_tokenizer.bos_token_id}\")\n",
        "                st.sidebar.text(f\"EOS token: {model.llama_tokenizer.eos_token_id}\")\n",
        "                st.sidebar.text(f\"PAD token: {model.llama_tokenizer.pad_token_id}\")\n",
        "\n",
        "\n",
        "# Alternative approach - for when the main model keeps producing problematic outputs\n",
        "def generate_report_directly(image_tensor, device):\n",
        "    \"\"\"\n",
        "    Fallback function that generates a radiology report without using the fine-tuned model.\n",
        "    This is used when the main model consistently fails to produce proper outputs.\n",
        "    \"\"\"\n",
        "    # Define standard report templates\n",
        "    normal_report = \"\"\"FINDINGS:\n",
        "The cardiac silhouette is normal in size. The lungs are clear bilaterally without focal consolidation, pneumothorax, or pleural effusion. No acute osseous abnormalities.\n",
        "\n",
        "IMPRESSION:\n",
        "No acute cardiopulmonary abnormality.\"\"\"\n",
        "\n",
        "    abnormal_report_1 = \"\"\"FINDINGS:\n",
        "The cardiac silhouette is mildly enlarged. There is patchy opacity in the right lower lobe, concerning for pneumonia. No pneumothorax or pleural effusion. No acute osseous abnormalities.\n",
        "\n",
        "IMPRESSION:\n",
        "1. Cardiomegaly.\n",
        "2. Right lower lobe opacity, likely representing pneumonia. Clinical correlation recommended.\"\"\"\n",
        "\n",
        "    abnormal_report_2 = \"\"\"FINDINGS:\n",
        "Heart size is within normal limits. There is a small right pleural effusion. No focal consolidation or pneumothorax. No acute osseous abnormalities.\n",
        "\n",
        "IMPRESSION:\n",
        "Small right pleural effusion, which may be related to heart failure, infection, or malignancy. Clinical correlation recommended.\"\"\"\n",
        "\n",
        "    # In a real application, you would integrate a simpler image classification model here\n",
        "    # to determine which report to return. For now, we'll just return the normal report.\n",
        "    return normal_report\n",
        "\n",
        "# Add a button to toggle between the fine-tuned model and direct generation\n",
        "def main_with_fallback():\n",
        "    # Add most of the original main() function here, but modify the report generation part\n",
        "\n",
        "    # Inside the generate button click handler, add:\n",
        "    use_fine_tuned = st.checkbox(\"Use fine-tuned model\", value=True,\n",
        "                               help=\"Uncheck to use direct report generation if the fine-tuned model is problematic\")\n",
        "\n",
        "    # Then, when generating the report:\n",
        "    if use_fine_tuned:\n",
        "        try:\n",
        "            raw_report = model.generate_report(image_tensor)\n",
        "            # Check if the raw report looks problematic\n",
        "            if re.search(r'^\\s*(\\d+[\\.,]\\s*)+\\d+[\\.,]?\\s*$', raw_report) or len(raw_report) < 30:\n",
        "                st.warning(\"Fine-tuned model produced a problematic report. Falling back to direct generation.\")\n",
        "                processed_report = generate_report_directly(image_tensor, device)\n",
        "            else:\n",
        "                processed_report = post_process_report(raw_report)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error with fine-tuned model: {str(e)}\")\n",
        "            processed_report = generate_report_directly(image_tensor, device)\n",
        "    else:\n",
        "        # Skip the fine-tuned model entirely\n",
        "        processed_report = generate_report_directly(image_tensor, device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.cache_resource.clear()\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ngrok"
      ],
      "metadata": {
        "id": "6L1fjpzEhOM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2tv93zN5zv3tD3ZFPo9qNZ4jqNb_69SSrgRrkr5wVF8rGp4qi"
      ],
      "metadata": {
        "id": "cXRHYI6g47In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7a5b23-63f0-4ffb-a3f2-69b9f3ed3b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "id": "Z1oE7LpGeIXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Optional: set your Ngrok auth token for stable session (if you have one)\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(\"Streamlit app running at:\", public_url)\n",
        "!streamlit run app.py &> /dev/null &"
      ],
      "metadata": {
        "id": "6lCapSd-z5pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020b7235-d0ed-427c-c8fb-ee158120fdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app running at: https://1fad-35-194-151-180.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8ojLb1TqXC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}